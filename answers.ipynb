{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got the ouputs as\n",
    "``` k\n",
    "Gradient with respect to θ0 (theta_0): -7.447054386138916\n",
    "Gradient with respect to θ1 (theta_1): -1.0253016948699951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output we got is\n",
    "```k\n",
    "Average Stochastic Gradient for θ0: -7.170480623841286\n",
    "Average Stochastic Gradient for θ1: -1.044655091315508\n",
    "True Gradient for θ0: -7.447054386138916\n",
    "True Gradient for θ1: -1.0253016948699951\n",
    "Difference in θ0 gradient: 0.2765737622976303\n",
    "Difference in θ1 gradient: 0.019353396445512816\n",
    "The stochastic gradient is a good estimate if these small differences are acceptable in the context of the problem. Generally, in stochastic gradient descent (SGD), it's expected that the stochastic gradient may not exactly match the true gradient but should be close enough to guide optimization in the correct direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got the average iterations for each method as\n",
    "```k\n",
    "Average iterations required for Full-batch method to reach minimum loss are: 1.0 Number of epochs 655\n",
    "Average iterations required for Mini-batch method to reach minimum loss are: 8.0 Number of epochs 8\n",
    "Average iterations required for SGD method to reach minimum loss are: 40.0 Number of epochs 4668\n",
    "In this case more Number of epochs are taken for sgd>full_batch>mini_batch\n",
    "The, full-batch gradient descent converges with the fewest epochs but requires more computational effort per iteration.\n",
    "The mini-batch method reaches the minimum loss quickly in terms of the number of epochs because the gradient estimates are less noisy than SGD and the smaller batches enable faster updates compared to full-batch.\n",
    "In stochastic gradient descent, each iteration corresponds to using just one sample to update the model, which leads to much noisier gradient estimates.\n",
    "While SGD has a much larger number of iterations, each iteration is much faster because only one data point is used. Therefore, SGD requires the most epochs to converge. In Stochastic Gradient Descent, each update is based on the gradient computed from a single data point. This leads to very noisy gradient estimates because a single data point might not represent the overall trend in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average Number of iterations and total number of epochs for each method is given below\n",
    "```k\n",
    "Steps with Full-Batch Gradient Descent with Momentum: 1.0  Number of epochs:38\n",
    "Steps with Stochastic Gradient Descent with Momentum: 40.0  Number of epochs:493\n",
    "Vanilla Full-Batch GD Steps: 1.0  Number of epochs:655\n",
    "Vanilla Stochastic GD Steps: 40.0  Number of epochs:4183\n",
    "Vanilla GD (both full-batch and stochastic) updates only based on the current gradient. In regions where the surface is flat or noisy (in the case of SGD), this can result in small updates or even oscillations that slow down convergence.\n",
    "\n",
    "GD with Momentum introduces a term that \"remembers\" the previous updates. This allows momentum to accelerate the gradient in consistent directions and smooth out oscillations, leading to faster convergence, particularly in cases where vanilla GD would slow down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Had run the code for 5000 epochs and got losses as\n",
    "```k\n",
    "\n",
    "Resized Image shape: torch.Size([3, 500, 500])\n",
    "Epoch [0/5000], Loss: 0.078863\n",
    "Epoch [500/5000], Loss: 0.067340\n",
    "Epoch [1000/5000], Loss: 0.062392\n",
    "Epoch [1500/5000], Loss: 0.059125\n",
    "Epoch [2000/5000], Loss: 0.056701\n",
    "Epoch [2500/5000], Loss: 0.054787\n",
    "Epoch [3000/5000], Loss: 0.053219\n",
    "Epoch [3500/5000], Loss: 0.051899\n",
    "Epoch [4000/5000], Loss: 0.050767\n",
    "Epoch [4500/5000], Loss: 0.049781\n",
    "MSE: 0.048909\n",
    "PSNR: 13.11 dB\n",
    "In this case, the MSE  calculated was 0.048909, which is relatively small. This suggests that on average, the difference in pixel values between the original and reconstructed images is minimal.\n",
    "In thia case, the PSNR is 13.11 dB, which is relatively low. Generally, a PSNR of around 30-40 dB is considered good for image reconstruction. A PSNR around 13 dB suggests noticeable artifacts or blurring in the reconstructed image, indicating that the RFF+Linear Regression approach might not perfectly capture high-frequency details in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reconstructing the audio I got the below outputs\n",
    "```k\n",
    "RMSE: 0.137688\n",
    "SNR: 2.37 dB\n",
    "An RMSE of 0.137688 suggests some discrepancy between the original and reconstructed audio, but whether this is acceptable depends on the context (e.g., audio quality requirements).\n",
    "An SNR of 2.37 dB indicates that the reconstruction has significant noise compared to the signal. Higher SNR values are preferable, typically above 10 dB for acceptable quality."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
